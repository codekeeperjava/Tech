## 几个概念

- index : 可以理解为数据库中的 table，是组合一类数据结构的集合。
- type : 新版本已经计划取消 type，比如我们有一个订单表，又可以分为虚拟商品订单表和实物订单表，其中有少许不同，则可以使用 type 去区分。
- mapping : index 中数据的规范，用来存储 document 中字段的类型。
- document : 一条记录，可以理解为数据库中的一行记录。
- shard : ES 会将 index 的数据以 shard 的集合分开存储，因为 ES 底层使用 lucene 实现，它们的对应关系是一个 shard 对应一个 lucene 索引。

## 分布式原理

![2020083119RgmspY34150e4c.png](http://img.dotleo.cn/blog/2020083119RgmspY34150e4c.png)

一个 ES 实例就是一个 node，一个机器上可以运行多个 node，但大部分情况下一个机器上都只运行一个实例(Node)。集群 Cluster 是由多个 node 组成，这里面会有一个是 master node，负责维护集群信息等。

每个索引 index 可以分为多个 shard，其中有一个是 primary shard，其他的都为 replica shard。写时只能通过 primary shard，读时可以通过 primary shard 和 replica shard。

当一个 node 挂掉后，master node 会将这个 node 中原本的 primary shard 指定到其他 node 上。当 master node 挂掉后，会重新选 master node，重启后将变为普通的 node。

## 为什么 ES 是准实时的？ES 的写入流程

### 写入

主要写入流程为：

1. Client 通过任一结点连接，连接后该结点作为协调结点
2. 协调结点通过 hash 取余的方式算得真正 primary shard 存在的结点并将请求传递给该结点
3. 该结点进行写入操作，具体写入步骤如下所示，写入完成后返回响应给协调结点，然后由协调结点返回给 Client

```
primary shard -> memory buffer
              -> translog

memory buffer -> 每 1 秒会 refresh 一次
              -> 满了也会 refresh 一次
                    -> refresh 后会暂存到 OS cache

translog      -> 每 30 分钟会 flush 一次
              -> 超过一定大小也会 flush 一次
                    -> 1. 写 commit point
                    -> 2. 将 OS cache 中的数据 fsync 强刷到磁盘 segment 中
                    -> 3. 清空 translog

segment       == 数据最终被持久化到 segment
```

整体流程如下图：

![20200901189F8Pzf1a7b4387.png](http://img.dotleo.cn/blog/20200901189F8Pzf1a7b4387.png)

**translog** 作用在于，当 memory buffer 还没有 refresh 或者 OS cache 还没有 fsync 时，机器故障重启等会丢失数据，但 translog 是存储在磁盘中，机器重启后可以用来恢复数据。但通过图中可以看出，translog 会先写入到 OS cache 中，因此如果此时服务器出现故障，丢失内存数据后，该 OS cache 中的 translog 也将无法恢复，导致 **数据丢失** 。

注：在数据写入到 Memory buffer 时，是无法被查询的，只有当被 refresh 到 OS cache 及之后的流程中才能被查询到，因此我们说 **ES 是准实时的** 。

### 删除

上面的流程外，还有一个 merge 流程。因为此前会产生多个 segment ，ES 后台会定期 merge 这些 segment，让它们变为一个更大的 segment。

当删除一个 document 时，ES 会先将该 document 相关信息写入到一个 `.DEL` 文件中，每次客户端的搜索都会去查询该文件过滤结果。而真正的落盘，就是在 merge 的时候，会将 `.DEL` 中的 document 进行删除。

![2020090118FJSmsV09d4e642.png](http://img.dotleo.cn/blog/2020090118FJSmsV09d4e642.png)

## 查询

查询可以分为两种，*按 id 查询*、*搜索*。

### 按 id 查询

插入数据后，会生成 docid （也可以手动指定），查询时通过 docid 进行 hash，然后找到 primary shard 后查找数据，该方式非常快。

对于按 id 的 GET 查询，ES 是实时的，因为它会先查询 Cache 中的 translog，如果有就返回。接着会查询磁盘中的 translog，如果有就返回。接着才会查询 segment。因此可以认为 ES 按照 id 的查询是 **实时的**。

### 搜索

搜索时，协调结点会将请求发送到每个不同的 shard （可能是 primary shard 和 replica shard 中其一，主要看设置），每个 shard 会查询 TOP N 的结果返回给协调结点，由协调结点进行 merge 后返回结果。

对于搜索，ES 是准实时的，因为它会先查询 Cache 中的 segment，如果有就返回。接着查询磁盘中的 segment。如果写入到内存 Buffer 中的数据是不能被查询到，因此搜索是 **准实时的**。

对于比如 match 这样的匹配查询，其步骤大概如下：

1. 检查字段类型。标题 `title` 字段是一个 `string` 类型（ `analyzed` ）已分析的全文字段，这意味着查询字符串本身也应该被分析。
1. 分析查询字符串。以查询 `QUICK!` 为例，将查询的字符串 `QUICK!` 传入标准分析器中，输出的结果是单个项 `quick` 。因为只有一个单词项，所以 `match` 查询执行的是单个底层 `term` 查询。
1. 查找匹配文档。用 `term` 查询在倒排索引中查找 `quick` 然后获取一组包含该项的文档。
1. 为每个文档评分。用 `term` 查询计算每个文档相关度评分 `_score` ，这是种将词频（term frequency，即词 `quick` 在相关文档的 `title` 字段中出现的频率）和反向文档频率（inverse document frequency，即词 `quick` 在所有文档的 `title` 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式。
1. 返回 TOP N 的结果。

## 大数据量如何优化

### 性能优化杀手锏 - filesystem cache

也就是 filesystem cache，操作系统缓存。

- 增大 filesystem cache 的容量，因为内存查询效率高于磁盘很多，所以在查询结束后会被缓存在 filesystem Cache 中，只要 filesystem cache 足够大，就能缓存足够多的数据来提高速度。
- 减少存储字段，这样每次检索所扫描的文件就比较少。

### 数据预热

举个例子：比如微博，可以把一些大 V，平时看得人多的数据提前给自己后台搞个系统，每隔一会儿，去自己系统搜索一下热数据，将数据刷到 filesystem cache，后面的用户搜索时直接通过 file systemcache 搜索，会很快。

### document 模型设计

在 MySQL 表设计中，经常会将表按照范式进行拆分，比如将订单表和收获地址表拆分开，但在 ES 中多个索引查询等复杂语句必然不能带来很高的查询效率，因此需要对 document 进行重新设计，比如在一条 document 中同时包含订单表和收获地址表中有需要的几个字段。

es 的分页是较坑的, 为啥呢?举个例子吧, 假如你毎页是 10 条数据, 你现在要查询第 100 页, 实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上, 如果你有个 5 个 shard, 那么就有 500 条数据, 接着协调节点对这 500 条数据进行一些合并、处理, 再获取到最终第 100 页的 10 条数据。

### 分页

- 不允许深分页 / 默认分页性能很惨，因为对于深翻页，比如每页 100 条，第 10 页，此时 ES 需要从每个 shard 中查询 TOP 1000 并发送给协调结点，这中间的工作量是很大的。
- 使用 scroll 进行遍历，实现类似微博的信息流，只能向后翻页，不能随意跳转。

## 生产环境集群部署情况

总共 7000 万 document，每日新增 1 万，目前 ES 集群数据量在 200 GB 左右。

5 台机器，每台 32G 内存，集群总内存 160 GB左右。

大概有 7 个索引，每个索引 5 个 shard 2 副本。
